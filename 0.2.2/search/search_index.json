{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Repid Repid is a job queuing library for Python with focus on simplicity. Example Here's what a smallest (but already working) peace of code can look like: import repid import asyncio from aioredis import Redis # create redis instance as usual myredis = Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) # create repid worker using redis instance myworker = repid . Worker ( myredis ) # add a simple actor to the worker @myworker . actor () async def my_first_job (): return \"Hello Repid!\" # enqueue a job... asyncio . run ( myworker . enqueue_job ( \"my_first_job\" )) # ...and run the worker asyncio . run ( myworker . run_forever ()) Install Repid supports Python versions 3.8 and up and installable via either pip or poetry . pip install repid poetry add repid Why repid? Asyncio Repid is built around asyncio . It means it's pretty fast. And you don't have to worry that it will slow down your other asyncio-driven code. Ease of integration You are probably already used to aioredis . Why don't you utilize it? Just pass its instance to repid and you are ready to go! Built with microservices in mind Your producer and consumer can be running in different containers, repid will handle it just fine. Can be used with other languages Repid doesn't use pickling, instead all data is parsed into json, which are documented here . It means you can easily adopt it for other languages. Integrated scheduling Repid has its own scheduling mechanisms. You can delay job execution until some date or even execute it every once in a while. No need for extra dependencies! Small Repid tries to be as small as possible. It makes it simpler to learn, simpler to test and simpler to use. So what are you waiting for? Inspiration Repid is inspired by dramatiq and arq . License This project is licensed under the terms of the MIT license.","title":"Home"},{"location":"#repid","text":"Repid is a job queuing library for Python with focus on simplicity.","title":"Repid"},{"location":"#example","text":"Here's what a smallest (but already working) peace of code can look like: import repid import asyncio from aioredis import Redis # create redis instance as usual myredis = Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) # create repid worker using redis instance myworker = repid . Worker ( myredis ) # add a simple actor to the worker @myworker . actor () async def my_first_job (): return \"Hello Repid!\" # enqueue a job... asyncio . run ( myworker . enqueue_job ( \"my_first_job\" )) # ...and run the worker asyncio . run ( myworker . run_forever ())","title":"Example"},{"location":"#install","text":"Repid supports Python versions 3.8 and up and installable via either pip or poetry . pip install repid poetry add repid","title":"Install"},{"location":"#why-repid","text":"","title":"Why repid?"},{"location":"#asyncio","text":"Repid is built around asyncio . It means it's pretty fast. And you don't have to worry that it will slow down your other asyncio-driven code.","title":"Asyncio"},{"location":"#ease-of-integration","text":"You are probably already used to aioredis . Why don't you utilize it? Just pass its instance to repid and you are ready to go!","title":"Ease of integration"},{"location":"#built-with-microservices-in-mind","text":"Your producer and consumer can be running in different containers, repid will handle it just fine.","title":"Built with microservices in mind"},{"location":"#can-be-used-with-other-languages","text":"Repid doesn't use pickling, instead all data is parsed into json, which are documented here . It means you can easily adopt it for other languages.","title":"Can be used with other languages"},{"location":"#integrated-scheduling","text":"Repid has its own scheduling mechanisms. You can delay job execution until some date or even execute it every once in a while. No need for extra dependencies!","title":"Integrated scheduling"},{"location":"#small","text":"Repid tries to be as small as possible. It makes it simpler to learn, simpler to test and simpler to use. So what are you waiting for?","title":"Small"},{"location":"#inspiration","text":"Repid is inspired by dramatiq and arq .","title":"Inspiration"},{"location":"#license","text":"This project is licensed under the terms of the MIT license.","title":"License"},{"location":"user_guide/","text":"User Guide To follow this guide you need a running instance of Redis and virtual environment with Repid installed. Worker Let's write a simple pseudo-async function that counts length of a string: import asyncio async def string_length ( the_string : str ) -> int : await asyncio . sleep ( 1 ) return len ( the_string ) Now, to add queuing and RPC functionality to this function all we have to do is: import asyncio import aioredis import repid redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) worker = repid . Worker ( redis ) @worker . actor () async def string_length ( the_string : str ) -> int : await asyncio . sleep ( 1 ) return len ( the_string ) asyncio . run ( worker . run_forever ()) Important All input and return arguments must be JSON-encodable. The example above is a totally valid RPC server but what if we want to use sync code? Let's try this out! import asyncio import aioredis import repid redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) worker = repid . Worker ( redis ) @worker . actor () async def string_length ( the_string : str ) -> int : await asyncio . sleep ( 1 ) return len ( the_string ) @worker . actor () def count_file_length () -> int : with open ( \"myfile.txt\" ) as f : return len ( f . readline ()) asyncio . run ( worker . run_forever ()) Every time count_file_length function will be called, it will be executed in a separate thread using ThreadPoolExecutor , so it won't stop other async code. Producer Simplest example to enqueue a job is: import asyncio import aioredis import repid redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) producer = repid . Repid ( redis ) asyncio . run ( producer . enqueue_job ( \"my_func_name\" )) This will enqueue a job for the func my_func_name within queue default . Queuing Type of queue Repid realizes FIFO (First In - First Out) queues. Repid tries to avoid receiving one job twice by different workers. To do so it pulls out a job from the queue before getting final decision will the job be done or not. If something goes wrong during this process there may be a situation when execution of the job was missed. Deferred tasks Actually, there are 2 queues under the hood. One is for normal tasks, and another one is for deferred. Deferred tasks have priority over normal. Unexisting actor If a job was taken from the queue but not found on the worker it will be re-enqueued. You should avoid these situations, however it may be helpful if you are doing canary release or refactoring. Non-default queue If a queue is not mentioned default queue will be used. import asyncio import aioredis import repid from datetime import timedelta redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) producer = repid . Repid ( redis ) asyncio . run ( await producer . enqueue_job ( \"hello_another_queue\" , queue = \"myqueue\" )) import asyncio import aioredis import repid redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) worker = repid . Worker ( redis ) @worker . actor ( queue = \"myqueue\" ) async def hello_another_queue (): print ( \"hi\" ) asyncio . run ( worker . run_forever ()) Error handling Let's say you have an actor that throws exception from time to time. @worker . actor () async def exceptional_actor ( i : int ) -> int : if i == 3 : raise Exception return i By default it will run once and if it fails... well, it fails. However, you can configure number of retries: @worker . actor ( retries = 3 ) async def exceptional_actor ( i : int ) -> int : if i == 3 : raise Exception return i Now, if it fails it will retry specified number of times. Important Repid doesn't return job back to the queue during retries, so the job is always executed on the same worker. Scheduling You can defer your job's execution until or by some time. To do so, follow examples below. Defer job until 1 January 2020 import asyncio import aioredis import repid from datetime import datetime redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) producer = repid . Repid ( redis ) async def main (): await producer . enqueue_job ( \"happy_new_year\" , defer_until = datetime ( 2020 , 1 , 1 )) asyncio . run ( main ()) Run job every day import asyncio import aioredis import repid from datetime import timedelta redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) producer = repid . Repid ( redis ) async def main (): await producer . enqueue_job ( \"good_morning\" , defer_by = timedelta ( days = 1 )) asyncio . run ( main ()) Results Every job will have result stored even if job's function returns none. You can access it with result property on the job. import asyncio import aioredis import repid from datetime import timedelta redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) producer = repid . Repid ( redis ) async def main (): myjob = await producer . enqueue_job ( \"some_job\" ) await asyncio . sleep ( 5 ) # wait for the job to complete result : repid . JobResult = await myjob . result print ( result ) asyncio . run ( main ())","title":"User Guide"},{"location":"user_guide/#user-guide","text":"To follow this guide you need a running instance of Redis and virtual environment with Repid installed.","title":"User Guide"},{"location":"user_guide/#worker","text":"Let's write a simple pseudo-async function that counts length of a string: import asyncio async def string_length ( the_string : str ) -> int : await asyncio . sleep ( 1 ) return len ( the_string ) Now, to add queuing and RPC functionality to this function all we have to do is: import asyncio import aioredis import repid redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) worker = repid . Worker ( redis ) @worker . actor () async def string_length ( the_string : str ) -> int : await asyncio . sleep ( 1 ) return len ( the_string ) asyncio . run ( worker . run_forever ()) Important All input and return arguments must be JSON-encodable. The example above is a totally valid RPC server but what if we want to use sync code? Let's try this out! import asyncio import aioredis import repid redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) worker = repid . Worker ( redis ) @worker . actor () async def string_length ( the_string : str ) -> int : await asyncio . sleep ( 1 ) return len ( the_string ) @worker . actor () def count_file_length () -> int : with open ( \"myfile.txt\" ) as f : return len ( f . readline ()) asyncio . run ( worker . run_forever ()) Every time count_file_length function will be called, it will be executed in a separate thread using ThreadPoolExecutor , so it won't stop other async code.","title":"Worker"},{"location":"user_guide/#producer","text":"Simplest example to enqueue a job is: import asyncio import aioredis import repid redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) producer = repid . Repid ( redis ) asyncio . run ( producer . enqueue_job ( \"my_func_name\" )) This will enqueue a job for the func my_func_name within queue default .","title":"Producer"},{"location":"user_guide/#queuing","text":"","title":"Queuing"},{"location":"user_guide/#type-of-queue","text":"Repid realizes FIFO (First In - First Out) queues. Repid tries to avoid receiving one job twice by different workers. To do so it pulls out a job from the queue before getting final decision will the job be done or not. If something goes wrong during this process there may be a situation when execution of the job was missed.","title":"Type of queue"},{"location":"user_guide/#deferred-tasks","text":"Actually, there are 2 queues under the hood. One is for normal tasks, and another one is for deferred. Deferred tasks have priority over normal.","title":"Deferred tasks"},{"location":"user_guide/#unexisting-actor","text":"If a job was taken from the queue but not found on the worker it will be re-enqueued. You should avoid these situations, however it may be helpful if you are doing canary release or refactoring.","title":"Unexisting actor"},{"location":"user_guide/#non-default-queue","text":"If a queue is not mentioned default queue will be used. import asyncio import aioredis import repid from datetime import timedelta redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) producer = repid . Repid ( redis ) asyncio . run ( await producer . enqueue_job ( \"hello_another_queue\" , queue = \"myqueue\" )) import asyncio import aioredis import repid redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) worker = repid . Worker ( redis ) @worker . actor ( queue = \"myqueue\" ) async def hello_another_queue (): print ( \"hi\" ) asyncio . run ( worker . run_forever ())","title":"Non-default queue"},{"location":"user_guide/#error-handling","text":"Let's say you have an actor that throws exception from time to time. @worker . actor () async def exceptional_actor ( i : int ) -> int : if i == 3 : raise Exception return i By default it will run once and if it fails... well, it fails. However, you can configure number of retries: @worker . actor ( retries = 3 ) async def exceptional_actor ( i : int ) -> int : if i == 3 : raise Exception return i Now, if it fails it will retry specified number of times. Important Repid doesn't return job back to the queue during retries, so the job is always executed on the same worker.","title":"Error handling"},{"location":"user_guide/#scheduling","text":"You can defer your job's execution until or by some time. To do so, follow examples below.","title":"Scheduling"},{"location":"user_guide/#defer-job-until-1-january-2020","text":"import asyncio import aioredis import repid from datetime import datetime redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) producer = repid . Repid ( redis ) async def main (): await producer . enqueue_job ( \"happy_new_year\" , defer_until = datetime ( 2020 , 1 , 1 )) asyncio . run ( main ())","title":"Defer job until 1 January 2020"},{"location":"user_guide/#run-job-every-day","text":"import asyncio import aioredis import repid from datetime import timedelta redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) producer = repid . Repid ( redis ) async def main (): await producer . enqueue_job ( \"good_morning\" , defer_by = timedelta ( days = 1 )) asyncio . run ( main ())","title":"Run job every day"},{"location":"user_guide/#results","text":"Every job will have result stored even if job's function returns none. You can access it with result property on the job. import asyncio import aioredis import repid from datetime import timedelta redis = aioredis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) producer = repid . Repid ( redis ) async def main (): myjob = await producer . enqueue_job ( \"some_job\" ) await asyncio . sleep ( 5 ) # wait for the job to complete result : repid . JobResult = await myjob . result print ( result ) asyncio . run ( main ())","title":"Results"}]}